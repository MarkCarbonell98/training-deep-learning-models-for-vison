{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day1/2_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on CIFAR10\n",
    "\n",
    "Now that we have seen how to load image data for classification and wrap it into a torch dataset, we will go ahead and train \"simple\" classiification models useing torch.\n",
    "In this notebook, we will start with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "that learns to classifiy the images based on the pixel values and then continue with a model trained on \"hard-coded\" convolutional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch and other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if we have gpu support.\n",
    "# colab offers free gpus, however they are not activated by default.\n",
    "# to activate the gpu, go to 'Runtime->Change runtime type'. \n",
    "# Then select 'GPU' in 'Hardware accelerator' and click 'Save'\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# we need to define the device for torch, yadda yadda\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we collect some functions we need from previous notebooks that we \n",
    "# want to reuse in 'utils.py'\n",
    "# TODO need to get this into colab\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that you have downloaded the\n",
    "# cifar dataset already, if not, rerun this:\n",
    "# !cifar2png cifar10 cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU USE THIS NOTEBOOK IN GOOGLE COLAB\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU USE THIS NOTEBOOK LOCALLY\n",
    "\n",
    "cifar_dir = './cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "We have already seen how to create a torch dataset for the cifar dataset in the previous notebook. We will repeat this here, but now split our data into training and validation data beforehand  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "images, labels = utils.load_cifar(os.path.join(cifar_dir, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split cifar into training and validation data:\n",
      "Have 42500 training images and 7500 validation images\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data, split into training and validation set\n",
    "\n",
    "# shuffle the data\n",
    "n_images = len(images)\n",
    "indices = np.arange(n_images)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# split into training and validation data\n",
    "validation_fraction = 0.15\n",
    "split_index = int(validation_fraction * n_images)\n",
    "train_indices = indices[:-split_index]\n",
    "val_indices = indices[-split_index:]\n",
    "\n",
    "train_images, val_images = images[train_indices], images[val_indices]\n",
    "train_labels, val_labels = labels[train_indices], labels[val_indices]\n",
    "assert len(train_images) == len(train_labels)\n",
    "assert len(val_images) == len(val_labels)\n",
    "assert len(train_images) + len(val_images) == n_images\n",
    "\n",
    "print(\"Split cifar into training and validation data:\")\n",
    "print(\"Have\", len(train_images), \"training images and\", len(val_images), \"validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the torch datasets for training and validation\n",
    "from functools import partial\n",
    "\n",
    "# TODO what is actually the clean way here? determine all on train?\n",
    "mean, std = np.mean(train_images), np.std(train_images)\n",
    "trafos = [\n",
    "    utils.to_channel_first,\n",
    "    partial(utils.normalize, mean=mean, std=std),\n",
    "    utils.to_tensor\n",
    "]\n",
    "trafos = partial(utils.compose, transforms=trafos)\n",
    "\n",
    "train_dataset = utils.DatasetWithTransform(train_images, train_labels,\n",
    "                                           transform=trafos)\n",
    "\n",
    "val_dataset = utils.DatasetWithTransform(val_images, val_labels,\n",
    "                                         transform=trafos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation function\n",
    "\n",
    "Now write the function that runs training and validation for 1 epoch.\n",
    "Epoch: training model on the training set once.\n",
    "Important: we want to monitor the progress during the training -> tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for tensorboard\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# this is the training function.\n",
    "# don't be scared by the many arguments, a lot of this is just\n",
    "# for logging, and we explain it in more detail in the\n",
    "# training procedure\n",
    "def train(model, loader, \n",
    "          loss_function, optimizer, device, epoch,\n",
    "          log_frequency=10, tb_logger=None,\n",
    "          log_image_interval=20):\n",
    "    \"\"\" Train model for one epoch.\n",
    "    \n",
    "    Arguments:\n",
    "        model - \n",
    "        loader - \n",
    "        loss_function - \n",
    "        optimizer - \n",
    "        device - \n",
    "        epoch - \n",
    "        log_frequency - \n",
    "        tb_logger - \n",
    "        log_image_interval -\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate over the training batches provided by the loader\n",
    "    n_batches = len(loader)\n",
    "    log_interval_ = n_batches // log_frequency\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "       \n",
    "        # send tensors to the active device\n",
    "        x.to(device), y.to(device)\n",
    "        \n",
    "        # set the gradients to zero, to start with \"clean\" gradients\n",
    "        # in this training iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # apply the model and get our prediction\n",
    "        prediction = model(x)\n",
    "        \n",
    "        # FIXME collate in dataloader is annoying\n",
    "        # calculate the loss (negative log likelihood loss)\n",
    "        loss_value = loss_function(prediction, y[:, 0])\n",
    "        \n",
    "        # calculate the gradients (`loss.backward()`) \n",
    "        # and apply them to the model weights (`optimizer.ste()`)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # report the training progress\n",
    "        if batch_id % log_interval_ == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_id * len(x), n_batches,\n",
    "                  100. * batch_id / n_batches, loss_value.item()))\n",
    "            \n",
    "        if tb_logger is not None:\n",
    "            step = epoch * n_batches + batch_id\n",
    "            tb_logger.add_scalar(tag='train-loss', \n",
    "                                 scalar_value=loss_value.item(),\n",
    "                                 global_step=step)\n",
    "            # TODO add the prediction and label here\n",
    "            if log_image_interval is not None and step % log_image_interval == 0:\n",
    "                tb_logger.add_images(tag='input', \n",
    "                                     img_tensor=x.to('cpu'),\n",
    "                                     global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yadda yadda\n",
    "def validate(model, loader, loss_function, device, \n",
    "             epoch, step, tb_logger=None):\n",
    "    n_batches = len(loader)\n",
    "    n_samples = len(loader.dataset)\n",
    "   \n",
    "    correct = 0\n",
    "    mean_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x.to(device), y.to(device)\n",
    "            prediction = model(x)\n",
    "            \n",
    "            # FIXME collate in dataloader is annoying\n",
    "            mean_loss += loss_function(prediction, y[:, 0]).item()\n",
    " \n",
    "            # compute the most likely class predictions\n",
    "            prediction = prediction.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(y.view_as(prediction)).sum().item()\n",
    "    \n",
    "    accuracy = correct / n_samples\n",
    "    mean_loss /= n_batches\n",
    "    print(\"Validate Epoch: {} Loss: {:.6f}, Accuracy: {:.6f}\".format(\n",
    "        epoch, mean_loss, accuracy\n",
    "    ))\n",
    "    if tb_logger is not None:\n",
    "        tb_logger.add_scalar(tag=\"validation-accuracy\",\n",
    "                             global_step=step,\n",
    "                             scalar_value=accuracy)\n",
    "        tb_logger.add_scalar(tag=\"validation-loss\",\n",
    "                             global_step=step,\n",
    "                             scalar_value=mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "different ways to define models, inherit from `torch.nn.Module`,\n",
    "define forward pass via method `forward`, yadda yadda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression model\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, n_pixels, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_pixels = n_pixels\n",
    "        self.n_classes = n_classes\n",
    "        self.log_reg = nn.Sequential(nn.Linear(self.n_pixels,\n",
    "                                               self.n_classes),\n",
    "                                     nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reshape the input to be 1d instead of 2d,\n",
    "        # which is required for fully connected layers\n",
    "        x = x.view(-1, self.n_pixels)\n",
    "        x = self.log_reg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the training\n",
    "\n",
    "Now it's time to bring everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressor(\n",
       "  (log_reg): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pixels = 3 * 32 * 32  # number channels * number pixels\n",
    "n_classes = 10    \n",
    "model = LogisticRegressor(n_pixels, n_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "# what is this yadda yadda\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "# what is this yadda yadda\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create loss\n",
    "# what is this yadda yadda\n",
    "loss_function = nn.NLLLoss()\n",
    "loss_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# TODO explain how to access the tensorboard\n",
    "tb_logger = SummaryWriter('runs/log_reg')\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/10625 (0%)]\tLoss: 193.064514\n",
      "Train Epoch: 0 [4248/10625 (10%)]\tLoss: 202.434006\n",
      "Train Epoch: 0 [8496/10625 (20%)]\tLoss: 471.633423\n",
      "Train Epoch: 0 [12744/10625 (30%)]\tLoss: 224.412338\n",
      "Train Epoch: 0 [16992/10625 (40%)]\tLoss: 111.017883\n",
      "Train Epoch: 0 [21240/10625 (50%)]\tLoss: 276.172302\n",
      "Train Epoch: 0 [25488/10625 (60%)]\tLoss: 570.844482\n",
      "Train Epoch: 0 [29736/10625 (70%)]\tLoss: 343.677063\n",
      "Train Epoch: 0 [33984/10625 (80%)]\tLoss: 173.348755\n",
      "Train Epoch: 0 [38232/10625 (90%)]\tLoss: 361.159454\n",
      "Train Epoch: 0 [42480/10625 (100%)]\tLoss: 283.078308\n",
      "Validate Epoch: 0 Loss: 306.538346, Accuracy: 0.240667\n",
      "Train Epoch: 1 [0/10625 (0%)]\tLoss: 275.397980\n",
      "Train Epoch: 1 [4248/10625 (10%)]\tLoss: 241.823090\n",
      "Train Epoch: 1 [8496/10625 (20%)]\tLoss: 293.156616\n",
      "Train Epoch: 1 [12744/10625 (30%)]\tLoss: 247.963928\n",
      "Train Epoch: 1 [16992/10625 (40%)]\tLoss: 650.058350\n",
      "Train Epoch: 1 [21240/10625 (50%)]\tLoss: 195.868988\n",
      "Train Epoch: 1 [25488/10625 (60%)]\tLoss: 100.993896\n",
      "Train Epoch: 1 [29736/10625 (70%)]\tLoss: 92.392151\n",
      "Train Epoch: 1 [33984/10625 (80%)]\tLoss: 77.763374\n",
      "Train Epoch: 1 [38232/10625 (90%)]\tLoss: 228.197266\n",
      "Train Epoch: 1 [42480/10625 (100%)]\tLoss: 445.522705\n",
      "Validate Epoch: 1 Loss: 263.778684, Accuracy: 0.289600\n",
      "Train Epoch: 2 [0/10625 (0%)]\tLoss: 89.649414\n",
      "Train Epoch: 2 [4248/10625 (10%)]\tLoss: 204.594803\n",
      "Train Epoch: 2 [8496/10625 (20%)]\tLoss: 345.369019\n",
      "Train Epoch: 2 [12744/10625 (30%)]\tLoss: 331.355286\n",
      "Train Epoch: 2 [16992/10625 (40%)]\tLoss: 59.007080\n",
      "Train Epoch: 2 [21240/10625 (50%)]\tLoss: 468.529358\n",
      "Train Epoch: 2 [25488/10625 (60%)]\tLoss: 144.833984\n",
      "Train Epoch: 2 [29736/10625 (70%)]\tLoss: 258.265198\n",
      "Train Epoch: 2 [33984/10625 (80%)]\tLoss: 179.517151\n",
      "Train Epoch: 2 [38232/10625 (90%)]\tLoss: 638.591675\n",
      "Train Epoch: 2 [42480/10625 (100%)]\tLoss: 280.179871\n",
      "Validate Epoch: 2 Loss: 327.833104, Accuracy: 0.256667\n",
      "Train Epoch: 3 [0/10625 (0%)]\tLoss: 268.755432\n",
      "Train Epoch: 3 [4248/10625 (10%)]\tLoss: 195.974182\n",
      "Train Epoch: 3 [8496/10625 (20%)]\tLoss: 314.411438\n",
      "Train Epoch: 3 [12744/10625 (30%)]\tLoss: 103.624557\n",
      "Train Epoch: 3 [16992/10625 (40%)]\tLoss: 50.892090\n",
      "Train Epoch: 3 [21240/10625 (50%)]\tLoss: 334.676239\n",
      "Train Epoch: 3 [25488/10625 (60%)]\tLoss: 462.156006\n",
      "Train Epoch: 3 [29736/10625 (70%)]\tLoss: 343.887451\n",
      "Train Epoch: 3 [33984/10625 (80%)]\tLoss: 142.024506\n",
      "Train Epoch: 3 [38232/10625 (90%)]\tLoss: 319.614014\n",
      "Train Epoch: 3 [42480/10625 (100%)]\tLoss: 1.759863\n",
      "Validate Epoch: 3 Loss: 274.149346, Accuracy: 0.256000\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "for epoch in range(n_epochs):\n",
    "    train(model, train_loader, loss_function, optimizer,\n",
    "          device, epoch, tb_logger=tb_logger)\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    validate(model, val_loader, loss_function, device, epoch, step,\n",
    "             tb_logger=tb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model on test data\n",
    "\n",
    "TODO need to check everything on test data!\n",
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the validation data\n",
    "val_images = torch.from_numpy(val_images)\n",
    "val_labels = torch.from_numpy(val_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            prediction = model(x)\n",
    "            # compute the most likely class predictions\n",
    "            prediction = prediction.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(y.view_as(prediction)).sum().item()\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3586666666666667\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = validate(model, val_loader)\n",
    "print(model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with preset filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this as a function that takes \n",
    "def apply_filters(image, target, filter_list, keep_image=False):    \n",
    "    filtered = [image] if keep_image else [] \n",
    "    for filter_function in filter_list:\n",
    "        filtered.append(filter_function(image))\n",
    "    data = np.concatenate(filtered, axis=-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.filters as filters\n",
    "\n",
    "# simple filter training\n",
    "filter_list = [filters.gaussian]\n",
    "blur_filter = partial(apply_filters,\n",
    "                      filter_list=filter_list,\n",
    "                      keep_image=True)\n",
    "\n",
    "\n",
    "trafos = [\n",
    "    blur_filter,\n",
    "    partial(utils.normalize, mean=mean, std=std),\n",
    "    utils.to_channel_first,\n",
    "    utils.to_tensor\n",
    "]\n",
    "trafos = partial(utils.compose, transforms=trafos)\n",
    "\n",
    "train_dataset = utils.DatasetWithTransform(train_images, train_labels,\n",
    "                                           transform=trafos)\n",
    "\n",
    "val_dataset = utils.DatasetWithTransform(val_images, val_labels,\n",
    "                                         transform=trafos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-94a7f6c16c16>:5: RuntimeWarning: Images with dimensions (M, N, 3) are interpreted as 2D+RGB by default. Use `multichannel=False` to interpret as 3D image with last dimension of length 3.\n",
      "  filtered.append(filter_function(image))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/10625 (0%)]\tLoss: 46.520142\n",
      "Train Epoch: 0 [4248/10625 (10%)]\tLoss: 443.046082\n",
      "Train Epoch: 0 [8496/10625 (20%)]\tLoss: 100.968681\n",
      "Train Epoch: 0 [12744/10625 (30%)]\tLoss: 277.865112\n",
      "Train Epoch: 0 [16992/10625 (40%)]\tLoss: 297.760437\n",
      "Train Epoch: 0 [21240/10625 (50%)]\tLoss: 676.625366\n",
      "Train Epoch: 0 [25488/10625 (60%)]\tLoss: 332.831421\n",
      "Train Epoch: 0 [29736/10625 (70%)]\tLoss: 324.362518\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bef8d3e7fb63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     train(model, train_loader, loss_function, optimizer,\n\u001b[0m\u001b[1;32m     13\u001b[0m           device, epoch, tb_logger=tb_logger, log_image_interval=None)\n\u001b[1;32m     14\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-6cfdaf788dac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, loss_function, optimizer, device, epoch, log_frequency, tb_logger, log_image_interval)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlog_interval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mlog_frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# send tensors to the active device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/software/conda/miniconda3/envs/adl-course/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/software/conda/miniconda3/envs/adl-course/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/software/conda/miniconda3/envs/adl-course/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/software/conda/miniconda3/envs/adl-course/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_pixels = 6 * 32 * 32  # number channels * number pixels\n",
    "n_classes = 10    \n",
    "model = LogisticRegressor(n_pixels, n_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "tb_logger = SummaryWriter('runs/log_reg')\n",
    "\n",
    "n_epochs = 4\n",
    "for epoch in range(n_epochs):\n",
    "    train(model, train_loader, loss_function, optimizer,\n",
    "          device, epoch, tb_logger=tb_logger, log_image_interval=None)\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    validate(model, val_loader, loss_function, device, epoch, step,\n",
    "             tb_logger=tb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks and Questions:\n",
    "\n",
    "Questions:\n",
    "- What accuracy do the different models reach?\n",
    "- Which accuracy do you expect by guessing?\n",
    "- Other metrics than accuracy / confusion table?\n",
    "\n",
    "Tasks:\n",
    "- Try more filters, compare the different models via tensorboard and on the test dataset.\n",
    "- Filters can be expressed as torch convolutions! Do this for torch.gaussian and train a model with this for different degrees of smoothing. How does it perform? (Give a hint!)\n",
    "- Advanced can you do it for other convolutional filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3.bkp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
