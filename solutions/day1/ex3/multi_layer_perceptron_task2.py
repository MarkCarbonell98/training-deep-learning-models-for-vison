# -*- coding: utf-8 -*-
"""Kopie von 3_multi_layer_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJqF7pXGFi635cIKMlz2GPOuSy_eDRDe

<a href="https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day1/3_multi_layer_perceptron.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Multi-layer Perceptron on CIFAR10

Based on the previous exercise, we will train a multi-layer network, also known as multi-layer perceptron. In this architecture, we will have 'hidden layers', i.e. layers that receive input not from the input directly but from previous layers in the network and that are not directly observed in the output.

## Preperation
"""

# Commented out IPython magic to ensure Python compatibility.
# load tensorboard extension # Marcos Carbonell version
# %load_ext tensorboard

# import torch and other libraries
import numpy as np
import time
import os
import matplotlib.pyplot as plt
import sys
sys.path.append(os.path.abspath('utils'))
import numpy as np
import sklearn
from tqdm import trange
from mlp import MLP
from mlp_sigmoid import MLP_Sigmoid
from mlp_more_layers import MLP_More_Layers
from mlp_softmax import MLP_Softmax
import tqdm
import torch

# check if we have gpu support
# colab offers free gpus, however they are not activated by default.
# to activate the gpu, go to 'Runtime->Change runtime type'. 
# Then select 'GPU' in 'Hardware accelerator' and click 'Save'
have_gpu = torch.cuda.is_available()
# we need to define the device for torch, yadda yadda
if have_gpu:
    print("GPU is available")
    device = torch.device('cuda')
else:
    print("GPU is not available, training will run on the CPU")
    device = torch.device('cpu')

# run this in google colab to get the utils.py file

# we will reuse the training function, validation function and
# data preparation from the previous notebook
import utils

cifar_dir = './cifar10'

categories = os.listdir('./cifar10/train')
categories.sort()

"""## Multi-layer perceptron

In this execise, we go from a single layer network to a network with multiple layers, also known as multi-layer perceptron, or MLP.
We still use fully connected layers (`nn.Linear`), i.e. each neuron in a given layer receives input from all neurons in the previous layer.

Imporantly, we apply a non-linearity to all layer outputs.
Otherwise, the layers could be collapsed into a single layer by matrix multiplication.
"""

train_dataset, val_dataset = utils.make_cifar_datasets(cifar_dir)

# Commented out IPython magic to ensure Python compatibility.
# instantiate loaders, loss, optimizer and tensorboard

train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader =    torch.utils.data.DataLoader(val_dataset, batch_size=25)
test_dataset = utils.make_cifar_test_dataset(cifar_dir)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=25)

loss_function = torch.nn.NLLLoss()
loss_function.to(device)

tb_logger = torch.utils.tensorboard.SummaryWriter('runs/log_mlp')
# %tensorboard --logdir runs

# train for a couple of epochs
fig1 = plt.figure(figsize=(35,35))
lr = 1.e-4
models = [(MLP(3072, 10), 'MLP LogSoftmax (Base)'), (MLP_Sigmoid(3072, 10), 'MLP LogSigmoid'), (MLP_Softmax(3072, 10), 'MLP Softmax'), (MLP_More_Layers(3072, 10), 'MLP More Layers')]
data = []
for i, (model, model_name) in enumerate(models):
    print(model, model_name)
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    tb_logger = torch.utils.tensorboard.SummaryWriter('runs/log_mlp_{model_name}'.format(model_name=model_name))
    n_epochs = 4
    start = int(time.time() * 1000)
    for epoch in trange(n_epochs):
        utils.train(model, train_loader, loss_function, optimizer,
                device, epoch, tb_logger=tb_logger)
        step = (epoch + 1) * len(train_loader)
        utils.validate(model, val_loader, loss_function,
                device, step,
                tb_logger=tb_logger)
    stop = int(time.time()  * 1000) - start
    print("Training time for MLP with model_name = {model_name}: {ttime}ms".format(model_name=model_name, ttime=stop))
    # evaluate the model on test data
    predictions, labels = utils.validate(model, test_loader, loss_function,
            device, step=0, tb_logger=None)

    accuracy = sklearn.metrics.accuracy_score(labels, predictions)
    print("Test accuracy: ", accuracy)
    data.append((stop, accuracy, lr))
    ax = fig1.add_subplot(2, 2, i+1)
    ax.set_title("{model_name} with learning rate {lr}".format(lr=lr, model_name=model_name))
    utils.make_confusion_matrix(labels, predictions, categories, ax)

# 3 different learning rates
times = [t[0] for t in data]
accs = [t[1] for t in data]
lrs = [t[2] for t in data]

plot_data = [times, accs, lrs]

plt.savefig(os.path.abspath(os.path.join('solutions', 'day1', 'ex3', 'ex3_task2_confusion_matrices.png')) ,format='png')

fig2 = plt.figure(figsize=(15,15))
width = .35
for i,(d, title) in enumerate(zip(plot_data, ['Training Time', 'Accuracy', 'Learning Rate'])):
    ax = fig2.add_subplot(1,3,i+1)
    x = np.arange(len(d))
    ax.bar(x - width/3, d, width, label = "lr = {lr}".format(lr=str(lrs[i])), )
    ax.set_title(title)
    ax.set_xticks(x)
    ax.set_xticklabels([str(el) for el in lrs])
    ax.set_xlabel("Learning Rate")

plt.savefig(os.path.abspath(os.path.join('solutions', 'day1', 'ex3', 'ex3_task2_model_data.png')) ,format='png')


plt.show()
"""## Tasks and Questions

Tasks
- Try 3 different learning rate values and observe how this changes the training curves, training time and results on the test data.
- Try different architectures for your MLP. For example, you can increase the number of layers or use a different activation function (e.g. Sigmoid) and compare them on the test dataset.

Question:
- Which of the models you have trained in this and in the previous notebook perform best?
- Besides the final performance, how do the training curves differ?
- Can you find any systematic similarities or differences in the confusion matrix for the different models?
- What can you do to make the results of these comparisons more reliable?

Advanced:
- Try one or more of your architectures with preset filters (see last notebook) as input features.
"""

"""
Answers:
Question 1: The MLP with the LogSoftmax activation function an layers just like the example MLP performed about 20% better on average than the other models with LogSigmoid, LogSoftmin and extra layers.

Question 2: 
- The loss of the original MLP started at about 2 and by the fourth epoch it had reached 1.75.
- The loss of the LogSigmoid MLP simply stagnated at 0 during the entire training. I think this happens because the input values were much smaller than one, making the logistic sigmoid function converge against zero since the first epoch
- The loss of the MLP using Softmin was negative the entire training. It started by -0.5 and ended by 0.75 in the fourth epoch. I do not know how to explain this behavior.
- The model with extra layers performed a bit better than the original MLP. Loss started at 1.75 and ended by almost 1.5. But still the accuracy of the model was much lower than the original MLP, and the training was almost ten times as much.

"""



plt.show()
