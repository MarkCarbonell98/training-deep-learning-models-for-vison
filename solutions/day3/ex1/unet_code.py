# -*- coding: utf-8 -*-
"""Kopie von day3 exercise 1 Unet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P-EoJkJT95_vNlGqIst-fpaz38_dRETr

## Training a Unet

In this notebook, we will train a 2D U-net for nuclei segmentation in the Kaggle Nuclei dataset.

It is still possible to do this exercise on the CPU, but you will need some patience to wait for the training. That's why we have added GPU support.
Please switch your Notebook to GPU in Edit -> Notebook Settings -> Hardware Accelerator.

Adapted from the pytorch example of Constantin Pape.

## The libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %load_ext tensorboard
import os
import time
import sys
sys.path.append(os.path.abspath('utils'))
import imageio
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import utils
import sklearn.metrics as metrics
import tqdm
import torchvision
from training_utils import train, validate
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.nn import functional as F
from torchvision import transforms
from nuclei_dataset import NucleiDataset
from random_crop import RandomCrop
from random_rotate import RandomRotate
from unet import UNet
from unet_batchnorm import UNet_BatchNorm
from unet_elu import UNet_ELU
from dice_coefficient import DiceCoefficient
from dice_loss import DiceLoss
from unet_five_layers import UNet_Five_Layers
"""## Data loading and preprocessing

For this exercise we will be using the Kaggle 2018 Data Science Bowl data again, but this time we will try to segment it with the state of the art network.
Let's start with loading the data as before.
"""

# !wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115" -O kaggle_data.zip && rm -rf /tmp/cookies.txt
# !unzip -qq kaggle_data.zip && rm kaggle_data.zip

"""Now make sure that the data was successfully extracted: if everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data` in your working directory. Check if it is the case:"""

# !ls -ltrh

"""__TASK__: Use `ls` to explore the contents of both folders. Running `ls your_folder_name` should display you what is stored in the folder of your interest.

 How are the images stored? What format do they have? What about the ground truth (the annotation masks)? Which format are they stored in?

Hint: you can use the following function to display the images:
"""



def show_one_image(image_path):
    image = imageio.imread(image_path)
    plt.imshow(image)

"""What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. In the previous exercises we did not have to worry about it, since we used the classic datasets available in the torchvision library. However, once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided by PyTorch, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.

Please take a moment to read about it [here](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).

The main idea: any Dataset class should have two methods: `len` that returns the dataset length (the number of element) and `getitem` that, given an index, returns input (image) and target (ground truth).

For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:
"""

#any PyTorch dataset class should inherit the initial torch.utils.data.Dataset
"""Now let's load the dataset and visualize it with a simple function:"""

TRAIN_DATA_PATH = './nuclei_train_data'
train_data = NucleiDataset(TRAIN_DATA_PATH)

def show_random_dataset_image(dataset):
    idx = np.random.randint(0, len(dataset))    # take a random sample
    img, mask = dataset[idx]                    # get the image and the nuclei masks
    f, axarr = plt.subplots(1, 2)               # make two plots on one figure
    axarr[0].imshow(img[0])                     # show the image
    axarr[1].imshow(mask[0])                    # show the masks
    _ = [ax.axis('off') for ax in axarr]        # remove the axes
    print('Image size is %s' % {img[0].shape})

show_random_dataset_image(train_data)
# implement
"""As you can probably see, if you clicked enough times, some of the images are really huge! What happens if we load them into memory and run the model on them? We might run out of memory. That's why normally, when training networks on images or volumes one has to be really careful about the sizes. In practice, you would want to regulate their size. Additional reason for restraining the size is: if we want to train in batches (faster and more stable training), we need all the images in the batch to be of the same size. That is why we prefer to either resize or crop them.

Here is a function (well, actually a class), that will apply a transformation 'random crop'. Notice that we apply it to images and masks simultaneously to make sure they correspond, despite the randomness.

In case anybody is wondering why we have to bother to write a whole class for it instead of simply coping the images directly in the dataset: we want to keep the code modular. We want to write one dataset object, and then we can try all the possible transforms with this one dataset. Similarly, we want to write one Randomcrop transform object, and then we can reuse it for any other image datasets we night have in the future.
"""

"""PS: PyTorch already has quite a bunch of all possible data transforms, so if you need one, check [here](https://pytorch.org/docs/stable/torchvision/transforms.html). The biggest problem with them is that they are clearly separated into transforms applied to PIL images (remember, we initially load the images as PIL.Image?) and torch.tensors (remember, we converted the images into tensors by calling transforms.ToTensor()?). This can be incredibly annoying if for some reason you might need to transorm your images to tensors before applying any other transforms or you don't want to use PIL library at all."""

train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))
train_loader = DataLoader(train_data, batch_size=5, shuffle=True)

show_random_dataset_image(train_data)

"""And the same for the validation data:"""

VAL_DATA_PATH = './nuclei_val_data'
val_data = NucleiDataset(VAL_DATA_PATH, RandomCrop(256))
val_loader = DataLoader(val_data, batch_size=5)

show_random_dataset_image(val_data)

"""## The model: U-net

Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.

The image of the model precisely describes all the building blocks you need to use to create it. All of them can be found in the list of PyTorch layers (modules) [here](https://pytorch.org/docs/stable/nn.html#convolution-layers).

The U-net has an encoder-decoder structure:

In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.

In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder.
"""

"""## Loss and distance metrics

The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html) we could use for the segmentation task.

Take a moment to think which one is better to use. If you are not sure, don't forget that you can always google! Before you start implementing the loss yourself, take a look at the [losses](https://pytorch.org/docs/stable/nn.html#loss-functions) already implemented in PyTorch. You can also look for implementations on GitHub.

__TASK__: implement your loss (or take one from pytorch):
"""


"""We will use the [Dice Coefficeint](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.
We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.
The Dice Coefficient is closely related to Jaccard Index / Intersection over Union.
"""

# sorensen dice coefficient implemented in torch
# the coefficient takes values in [0, 1], where 0 is
# the worst score, 1 is the best score
"""## Training

Let's start with writing training and validation functions. __TASK__: fix in all the TODOs to make the function run. You can use the function from the exercise2 as a template.
"""

"""This time we will use GPU to train faster. Please make sure that your Notebook is running on GPU."""

# check if we have  a gpu
if torch.cuda.is_available():
    print("GPU is available")
    device = torch.device("cuda")
else:
    print("GPU is not available")
    device = torch.device("cpu")

nets = [UNet(1,1, final_activation=nn.Sigmoid()), UNet_BatchNorm(1,1,final_activation=nn.Sigmoid()), UNet_ELU(1,1, final_activation=nn.Sigmoid()), UNet_Five_Layers(1,1, final_activation=nn.Sigmoid())]

n_epochs = 15
for net in nets:
    for loss_fn in [torch.nn.MSELoss(), DiceLoss(), DiceCoefficient()]:
        name = net.__class__.__name__
        loss_name = loss_fn.__class__.__name__
        print(f"\n\nTraining network {name} with loss {loss_name}")
        logger = SummaryWriter(f'runs/log_{name}_{loss_name}')
        net.to(device)


        loss_function = loss_fn
        loss_function.to(device)

        # use adam optimizer
        optimizer = torch.optim.Adam(net.parameters(), lr=1.e-3)

        # build the dice coefficient metric
        metric = DiceCoefficient()
        # train for 25 epochs
        start = int(time.time())
        stop = 0
        best_accuracy = 0.
        checkpoint_name='./best_checkpoint_{name}_{loss_name}.tar'.format(name=name, loss_name=loss_name)
        best_epoch = 0
        for epoch in tqdm.tqdm(range(n_epochs), total=n_epochs):
            # train
            train(net, train_loader, optimizer, loss_function, epoch, tb_logger=logger, device=device)

            step = epoch * len(train_loader.dataset)
            # validate
            _, acc = validate(net, val_loader, loss_function, metric, step=step, tb_logger=logger, device=device, optimizer=optimizer)

            if acc > best_accuracy:
                best_accuracy = acc
                best_epoch = epoch
                utils.save_checkpoint(net, optimizer, epoch, checkpoint_name)

        images, _ = next(iter(train_loader))
        images = images.to(device)
        grid = torchvision.utils.make_grid(images)
        logger.add_image('images', grid, 0)
        logger.add_graph(net, images)
        logger.close()

        stop = int(time.time()) - start
        print("Total training time {ttime} sec for network {name} {loss_name}".format(ttime=stop, name=name, loss_name=loss_name))
        print("Best checkpoint is epoch #", best_epoch+1, "saved in", checkpoint_name, "for network {name} with loss {loss_name}".format(name=name, loss_name=loss_name))

"""## Additional Exercises 

1. Implement and compare at least 2 of the following architecture variants of the U-Net:
    * use [Dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers) in the decoder path
    * use [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d) to normalize layer inputs
    * use [GroupNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.GroupNorm) to normalize convolutional group inputs
    * use [ELU-Activations](https://pytorch.org/docs/stable/nn.html#torch.nn.ELU) instead of ReLU-Activations

2. Use the Dice coefficient as loss function. Before we only used it for validation, but it is differentiable and can thus also be used as loss. Compare to the results from exercise 2. 

   - implement: The optimizer we use finds minima of the loss, but the minimal value for the Dice coefficient corresponds to a bad segmentation. How do we need to change the Dice coefficient to use it as loss nonetheless?

3. Add one more layer to the Unet model (currently it has 4). Compare the results.

## Advanced Exercises

1. Visualize the graph (model) that we are using with TensorBoard
2. Write your own data transform (e.g., RandomRotate)

"""
