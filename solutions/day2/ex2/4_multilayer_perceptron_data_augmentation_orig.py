# -*- coding: utf-8 -*-
"""3_multi_layer_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/2_data_augmentation.ipynb

<a href="https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/2_data_augmentation.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Data Augmentation on CIFAR10

In this exercise we will use data augmentation to increase the available training data and thus improve the network training performance.

## Preparation
"""

# Commented out IPython magic to ensure Python compatibility.
# load tensorboard extension
# %load_ext tensorboard

# import torch and other libraries
import os
import sys
sys.path.append(os.path.abspath('utils'))
import numpy as np
import sklearn.metrics as metrics
import matplotlib.pyplot as plt
import torch
import utils
import skimage.color as color
import functools
from cnn_batch_norm import CNNBatchNorm

# check if we have gpu support
# colab offers free gpus, however they are not activated by default.
# to activate the gpu, go to 'Runtime->Change runtime type'. 
# Then select 'GPU' in 'Hardware accelerator' and click 'Save'
have_gpu = torch.cuda.is_available()
# we need to define the device for torch, yadda yadda
if have_gpu:
    print("GPU is available")
    device = torch.device('cuda')
else:
    print("GPU is not available, training will run on the CPU")
    device = torch.device('cpu')

# run this in google colab to get the utils.py file

# we will reuse the training function, validation function and
# data preparation from the previous notebook

cifar_dir = './cifar10'

categories = os.listdir('./cifar10/train')
categories.sort()

images, labels = utils.load_cifar(os.path.join(cifar_dir, 'train'))
(train_images, train_labels,
        val_images, val_labels) = utils.make_cifar_train_val_split(images, labels)

"""## Data Augmentation

The goal of data augmentation is to increase the amount of training data by transforming the input images in a way that they still resemble realistic images, but differ from the input to the transformation.
Here, we will start with two transformations:
- random flips along the vertical centerline
- random color jitters
"""

# define random augmentations

def random_flip(image, target, probability=.5):
    """ Randomly mirror the image across the vertical axis.
    """
    if np.random.rand() < probability:
        image = np.array([np.fliplr(im) for im in image])
    return image, target

def random_color_jitter(image, target, probability=.5):
    """ Randomly jitter the saturation, hue and brightness of the image.
    """
    if np.random.rand() > probability:
        # skimage expects WHC instead of CHW
            image = image.transpose((1, 2, 0))
            # transform image to hsv color space to apply jitter
            image = color.rgb2hsv(image)
            # compute jitter factors in range 0.66 - 1.5  
            jitter_factors = 1.5 * np.random.rand(3)
            jitter_factors = np.clip(jitter_factors, 0.66, 1.5)
            # apply the jitter factors, making sure we stay in correct value range
            image *= jitter_factors
            image = np.clip(image, 0, 1)
            # transform back to rgb and CHW
            image = color.hsv2rgb(image)
            image = image.transpose((2, 0, 1))
    return image, target

# create training dataset with augmentations
train_trafos = [
        utils.to_channel_first,
        utils.normalize,
        random_color_jitter,
        random_flip,
        utils.to_tensor
        ]
train_trafos = functools.partial(utils.compose, transforms=train_trafos)

train_dataset = utils.DatasetWithTransform(train_images, train_labels,
        transform=train_trafos)

# we don't use data augmentations for the validation set
val_dataset = utils.DatasetWithTransform(val_images, val_labels,
        transform=utils.get_default_cifar_transform())

# sample augmentations
def show_image(ax, image):
    # need to go back to numpy array and WHC axis order
    image = image.numpy().transpose((1, 2, 0))
    ax.imshow(image)

n_samples = 8
image_id = 0
fig, ax = plt.subplots(1, n_samples, figsize=(18, 4))
for sample in range(n_samples):
    image, _ = train_dataset[0]
    show_image(ax[sample], image)

# we reuse the model from the previous exercise
# if you want you can also use a different CNN architecture that
# you have designed in the tasks part of that exercise
model = utils.SimpleCNN(10)
model = model.to(device)

# Commented out IPython magic to ensure Python compatibility.
# instantiate loaders and optimizer and start tensorboard
train_loader =torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader =torch.utils.data.DataLoader(val_dataset, batch_size=25)
optimizer =torch.optim.Adam(model.parameters(), lr=1.e-3)
# %tensorboard --logdir runs

# we have moved all the boilerplate for the full training procedure to utils now
if not os.path.exists("checkpoints/best_checkpoint_SimpleCNN.tar"):
    n_epochs = 10
    utils.run_cifar_training(model, optimizer,
            train_loader, val_loader,
            device=device, name='SimpleCNN', 
            n_epochs=n_epochs)

# evaluate the model on test data
test_dataset = utils.make_cifar_test_dataset(cifar_dir)
test_loader =torch.utils.data.DataLoader(test_dataset, batch_size=25)

model = utils.load_checkpoint("best_checkpoint_SimpleCNN.tar", model , optimizer)[0]

predictions, labels = utils.validate(model, test_loader,torch.nn.NLLLoss(),
        device, step=0, tb_logger=None)

print("Test accuracy:")
accuracy = metrics.accuracy_score(labels, predictions)
print(accuracy)

fig, ax = plt.subplots(1, figsize=(8, 8))
utils.make_confusion_matrix(labels, predictions, categories, ax)

"""## Normalization layers

In addition to convolutional layers and pooling layers, another important part of neural networks are normalization layers.

These layers keep their input normalized using a learned normalization. The first type of normalization introduced has been [BatchNorm](https://arxiv.org/abs/1502.03167), which we will now add to the CNN architecture from the previous exercise.
"""

# instantiate model and optimizer
model = CNNBatchNorm(10)
model = model.to(device)
optimizer =torch.optim.Adam(model.parameters(), lr=1.e-3)

if not os.path.exists("checkpoints/best_checkpoint_CNNBatchNorm.tar"):
    utils.run_cifar_training(model, optimizer,
            train_loader, val_loader,
            device=device, name='CNNBatchNorm', 
            n_epochs=n_epochs)

model = utils.load_checkpoint("best_checkpoint_CNNBatchNorm.tar", model, optimizer)[0]

predictions, labels = utils.validate(model, test_loader,torch.nn.NLLLoss(),
        device, step=0, tb_logger=None)

print("Test accuracy:")
accuracy = metrics.accuracy_score(labels, predictions)
print(accuracy)

fig, ax = plt.subplots(1, figsize=(8, 8))
utils.make_confusion_matrix(labels, predictions, categories, ax)

plt.show()

"""## Tasks and Questions

Tasks:
- Implement one or two additional augmentations and train the model again using these. You can use [the torchvision transformations](https://pytorch.org/docs/stable/torchvision/transforms.html) for inspiration.

Questions:
- Compare the model results in this exercise.
- Can you think of any transformations that make use of symmetries/invariances not present here but present in other kinds of images (e.g. biomedical images)?

Advanced:
- Check out the other [normalization layers available in pytorch](https://pytorch.org/docs/stable/nn.html#normalization-layers). Which layers could be beneficial to BatchNorm here? Try training with them and see if this improves performance further.
"""

