# -*- coding: utf-8 -*-
"""3_multi_layer_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/3_advanced_architectures.ipynb

<a href="https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/3_advanced_architectures.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Advanced Architectures on CIFAR10

Train more advanced architectures on CIFAR10.

## Preparation
"""

# Commented out IPython magic to ensure Python compatibility.
# load tensorboard extension
# %load_ext tensorboard

# import torch and other libraries
import os
import numpy as np
import sklearn.metrics as metrics
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import Adam

!pip install cifar2png

# check if we have gpu support
# colab offers free gpus, however they are not activated by default.
# to activate the gpu, go to 'Runtime->Change runtime type'. 
# Then select 'GPU' in 'Hardware accelerator' and click 'Save'
have_gpu = torch.cuda.is_available()
# we need to define the device for torch, yadda yadda
if have_gpu:
    print("GPU is available")
    device = torch.device('cuda')
else:
    print("GPU is not available, training will run on the CPU")
    device = torch.device('cpu')

# run this in google colab to get the utils.py file
!wget https://raw.githubusercontent.com/constantinpape/training-deep-learning-models-for-vison/master/day1/utils.py

# we will reuse the training function, validation function and
# data preparation from the previous notebook
import utils

cifar_dir = './cifar10'
!cifar2png cifar10 cifar10

categories = os.listdir('./cifar10/train')
categories.sort()

"""## Advanced Architectures

Torchvision offers implementations for several [common classification models](https://pytorch.org/docs/stable/torchvision/models.html).

Here, we will use the popular [ResNet](https://arxiv.org/abs/1512.03385). This architecture uses skip connections to improve gradient flow.
"""

from torchvision.models import resnet18
# load the smallest available resnet architecture (resnet18)
# and apply the LogSoftmax activation to its output to be compatible with our loss function
model = nn.Sequential(resnet18(num_classes=10), nn.LogSoftmax(dim=1))
model = model.to(device)

# get training and validation data
train_dataset, val_dataset = utils.make_cifar_datasets(cifar_dir)

# Commented out IPython magic to ensure Python compatibility.
# instantiate loaders, loss, optimizer and tensorboard
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=25)
optimizer = Adam(model.parameters(), lr=1.e-3)
# %tensorboard --logdir runs

n_epochs = 5
utils.run_cifar_training(model, optimizer,
                         train_loader, val_loader,
                         device=device, name='resnet18', 
                         n_epochs=n_epochs)

# evaluate the model on test data
test_dataset = utils.make_cifar_test_dataset(cifar_dir)
test_loader = DataLoader(test_dataset, batch_size=25)
predictions, labels = utils.validate(model, test_loader, nn.NLLLoss(),
                                     device, step=0, tb_logger=None)

import matplotlib.pyplot as plt
print("Test accuracy:")
accuracy = metrics.accuracy_score(labels, predictions)
print(accuracy)

fig, ax = plt.subplots(1, figsize=(8, 8))
utils.make_confusion_matrix(labels, predictions, categories, ax)

"""## Tasks and Questions

Tasks:
- Read up on some of the models in [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html) and train at least one of them on this data.
- Combine the best performing model in this exercise with data augmentation (previous exercise).

Questions:
- What's the best accuracy you have achieved on CIFAR10 over all the exercises? Which model and training procedure did lead to it?
- What would your next steps be to improve this performance?
- Do you think the performance possible on cifar will improve significantly with much larger models (= models with a lot more parameters)?
"""

