
# -*- coding: utf-8 -*-
"""3_multi_layer_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/3_advanced_architectures.ipynb

<a href="https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day2/3_advanced_architectures.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Advanced Architectures on CIFAR10

Train more advanced architectures on CIFAR10.

## Preparation
"""

# Commented out IPython magic to ensure Python compatibility.
# load tensorboard extension
# %load_ext tensorboard

# import torch and other libraries

import os
import sys
sys.path.append(os.path.abspath('utils'))
import numpy as np
import sklearn.metrics as metrics
import matplotlib.pyplot as plt
import functools
import skimage.color as color

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import Adam
from torchvision.models import resnet18
import utils


# check if we have gpu support
# colab offers free gpus, however they are not activated by default.
# to activate the gpu, go to 'Runtime->Change runtime type'. 
# Then select 'GPU' in 'Hardware accelerator' and click 'Save'
have_gpu = torch.cuda.is_available()
# we need to define the device for torch, yadda yadda
if have_gpu:
    print("GPU is available")
    device = torch.device('cuda')
else:
    print("GPU is not available, training will run on the CPU")
    device = torch.device('cpu')

# run this in google colab to get the utils.py file

# we will reuse the training function, validation function and
# data preparation from the previous notebook

cifar_dir = './cifar10'

categories = os.listdir('./cifar10/train')
categories.sort()

"""## Advanced Architectures

Torchvision offers implementations for several [common classification models](https://pytorch.org/docs/stable/torchvision/models.html).

Here, we will use the popular [ResNet](https://arxiv.org/abs/1512.03385). This architecture uses skip connections to improve gradient flow.
"""

# load the smallest available resnet architecture (resnet18)
# and apply the LogSoftmax activation to its output to be compatible with our loss function
# Commented out IPython magic to ensure Python compatibility.
# instantiate loaders, loss, optimizer and tensorboard
def random_flip(image, target, probability=.5):
    """ Randomly mirror the image across the vertical axis.
    """
    if np.random.rand() < probability:
        image = np.array([np.fliplr(im) for im in image])
    return image, target

def random_color_jitter(image, target, probability=.5):
    """ Randomly jitter the saturation, hue and brightness of the image.
    """
    if np.random.rand() > probability:
        # skimage expects WHC instead of CHW
            image = image.transpose((1, 2, 0))
            # transform image to hsv color space to apply jitter
            image = color.rgb2hsv(image)
            # compute jitter factors in range 0.66 - 1.5  
            jitter_factors = 1.5 * np.random.rand(3)
            jitter_factors = np.clip(jitter_factors, 0.66, 1.5)
            # apply the jitter factors, making sure we stay in correct value range
            image *= jitter_factors
            image = np.clip(image, 0, 1)
            # transform back to rgb and CHW
            image = color.hsv2rgb(image)
            image = image.transpose((2, 0, 1))
    return image, target

trafos = [
        utils.to_channel_first,
        utils.normalize,
        random_flip,
        random_color_jitter,
        utils.to_tensor
    ]

trafos = functools.partial(utils.compose, transforms=trafos)

model = nn.Sequential(resnet18(num_classes=10), nn.LogSoftmax(dim=1))
model = model.to(device)

# get training and validation data
images, labels = utils.load_cifar('./cifar10/train')
(train_images, train_labels, val_images, val_labels) = utils.make_cifar_train_val_split(images, labels)


train_dataset = utils.DatasetWithTransform(train_images, train_labels, transform=trafos)
val_dataset = utils.DatasetWithTransform(val_images, val_labels, transform=utils.get_default_cifar_transform())

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, )
val_loader = DataLoader(val_dataset, batch_size=25)
optimizer = Adam(model.parameters(), lr=1.e-3)
# %tensorboard --logdir runs

n_epochs = 5
utils.run_cifar_training(model, optimizer,
                         train_loader, val_loader,
                         device=device, name='resnet18_augmented', 
                         n_epochs=n_epochs)

# evaluate the model on test data
test_dataset = utils.make_cifar_test_dataset(cifar_dir)
test_loader = DataLoader(test_dataset, batch_size=25)
predictions, labels = utils.validate(model, test_loader, nn.NLLLoss(),
                                     device, step=0, tb_logger=None)

print("Test accuracy:")
accuracy = metrics.accuracy_score(labels, predictions)
print(accuracy)

fig, ax = plt.subplots(1, figsize=(8, 8))
utils.make_confusion_matrix(labels, predictions, categories, ax)

"""## Tasks and Questions

Tasks:
- Read up on some of the models in [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html) and train at least one of them on this data.
- Combine the best performing model in this exercise with data augmentation (previous exercise).

Questions:
- What's the best accuracy you have achieved on CIFAR10 over all the exercises? Which model and training procedure did lead to it?
- What would your next steps be to improve this performance?
- Do you think the performance possible on cifar will improve significantly with much larger models (= models with a lot more parameters)?
"""

